{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTING FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads files and creates list\n",
    "def filereader(path):\n",
    "    list_of_comments = []\n",
    "    all_files = os.listdir(path)\n",
    "    for folder in all_files:\n",
    "        files = os.listdir(path+folder)\n",
    "        for file in files:\n",
    "            pathname = path+folder+'\\\\'+file\n",
    "            file = open(pathname, 'r')\n",
    "            list_of_comments.append(file.readline())\n",
    "    return list_of_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pol_truthful = filereader(\"C:\\\\Users\\\\aligthar\\\\Desktop\\\\negative_polarity\\\\truthful_from_Web\\\\\")\n",
    "neg_pol_deceptive = filereader(\"C:\\\\Users\\\\aligthar\\\\Desktop\\\\negative_polarity\\\\deceptive_from_MTurk\\\\\")\n",
    "pos_pol_truthful = filereader(\"C:\\\\Users\\\\aligthar\\\\Desktop\\\\positive_polarity\\\\truthful_from_TripAdvisor\\\\\")\n",
    "pos_pol_deceptive = filereader(\"C:\\\\Users\\\\aligthar\\\\Desktop\\\\positive_polarity\\\\deceptive_from_MTurk\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING X2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = neg_pol_truthful + pos_pol_truthful + neg_pol_deceptive + pos_pol_deceptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_values = [1]*400 + [0]*400 + [1]*400 + [0]*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_words = [len(item.split()) for item in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_unique_words = [len(set(item.split())) for item in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sentences = [len(item.split('. ')) for item in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_no_words_per_sentence = []\n",
    "for item in all_files:\n",
    "    temp = []\n",
    "    sentences = item.split('. ')\n",
    "    for item in sentences:\n",
    "        temp += [len(item.split())]\n",
    "    avg_no_words_per_sentence += [mean(temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_digits_per_word = []\n",
    "for item in all_files:\n",
    "    temp = []\n",
    "    words = item.split()\n",
    "    for item in words:\n",
    "        temp += [len(item)]\n",
    "    no_digits_per_word += [mean(temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>no_words</th>\n",
       "      <th>no_unique_words</th>\n",
       "      <th>no_sentences</th>\n",
       "      <th>avg_no_words_per_sentence</th>\n",
       "      <th>no_digits_per_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>12.142857</td>\n",
       "      <td>4.047059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>116</td>\n",
       "      <td>10</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>4.503106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>137</td>\n",
       "      <td>12</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>4.150442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>207</td>\n",
       "      <td>159</td>\n",
       "      <td>6</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>4.120773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>8</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.491667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  no_words  no_unique_words  no_sentences  \\\n",
       "0         1        85               63             7   \n",
       "1         1       161              116            10   \n",
       "2         1       226              137            12   \n",
       "3         1       207              159             6   \n",
       "4         1       120               90             8   \n",
       "\n",
       "   avg_no_words_per_sentence  no_digits_per_word  \n",
       "0                  12.142857            4.047059  \n",
       "1                  16.100000            4.503106  \n",
       "2                  18.750000            4.150442  \n",
       "3                  34.500000            4.120773  \n",
       "4                  15.000000            4.491667  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFX2 = pd.DataFrame(\n",
    "    {'polarity': polarity_values,\n",
    "     'no_words': no_words,\n",
    "     'no_unique_words': no_unique_words,\n",
    "     'no_sentences': no_sentences,\n",
    "     'avg_no_words_per_sentence': avg_no_words_per_sentence,\n",
    "     'no_digits_per_word': no_digits_per_word\n",
    "    })\n",
    "DFX2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, pos_tags=False):\n",
    "    # tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]   \n",
    "\n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    # remove words less than three characters\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    # remove capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # lemmatizing\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # includes POS_tags\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "        tokens = ['_'.join(t) for t in tokens]\n",
    "    \n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text \n",
    "\n",
    "def preprocesser(list_of_texts): # turning lists into preprocessed lists\n",
    "    new = []\n",
    "    for item in list_of_texts:\n",
    "        new += [preprocessing(item)]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X1\n",
    "X1_neg_truthful = [r for r in preprocesser(neg_pol_truthful)]\n",
    "X1_neg_deceptive = [r for r in preprocesser(neg_pol_deceptive)]\n",
    "X1_pos_truthful = [r for r in preprocesser(pos_pol_truthful)]\n",
    "X1_pos_deceptive = [r for r in preprocesser(pos_pol_deceptive)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating X2\n",
    "X2 = DFX2.to_numpy()\n",
    "X2_neg_truthful = X2[:400]\n",
    "X2_neg_deceptive = X2[800:1200]\n",
    "X2_pol_truthful = X2[400:800]\n",
    "X2_pos_deceptive = X2[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining X1 and X2 and labelling data: 0 for truthful, 1 for deceptive\n",
    "corpus_neg_truthful = [(X1_neg_truthful[i],X2_neg_truthful[i],0) for i in range(len(X1_neg_truthful))]\n",
    "corpus_neg_deceptive = [(X1_neg_deceptive[i],X2_neg_deceptive[i],0) for i in range(len(X1_neg_deceptive))]\n",
    "corpus_pos_truthful = [(X1_pos_truthful[i],X2_pol_truthful[i],1) for i in range(len(X1_pos_truthful))]\n",
    "corpus_pos_deceptive = [(X1_pos_deceptive[i],X2_pos_deceptive[i],1) for i in range(len(X1_pos_deceptive))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_neg_truthful+corpus_neg_deceptive+corpus_pos_truthful+corpus_pos_deceptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CO-TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from co_training.sklearn_cotraining.classifiers import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from statistics import mean, stdev\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data and unpack X1 and X2\n",
    "from random import shuffle\n",
    "shuffle(corpus)\n",
    "X1 = [x1 for x1,x2,l in corpus]\n",
    "X2 = [x2 for x1,x2,l in corpus]\n",
    "X2 = [l.tolist() for l in X2]\n",
    "X2 = np.array(X2) #formatting\n",
    "labels_y = [l for x1,x2,l in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cotraining_CV(ratio, vectorizertype, ngram, nsplits, X1=X1):\n",
    "   \n",
    "    \"\"\"\n",
    "    Main function with cross-validation included\n",
    "    Parameters:\n",
    "        ratio (number of labels to remove) = float (0.80 / 0.90 / 0.95)\n",
    "        vectorizertype (word counts or TF-IDF values) = CountVectorizer / TfidfVectorizer\n",
    "        ngram (word pairs) = int (1=unigram, 2=bigram)\n",
    "        nsplits (cross validation splits) = int (number of splits)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize results dictionary \n",
    "    CT_SVM_results = {'accuracy':[], '1':{'precision':[], 'recall':[], 'f1-score':[]}}\n",
    "    CT_NB_results = {'accuracy':[], '1':{'precision':[], 'recall':[], 'f1-score':[]}}    \n",
    "    CT_RF_results = {'accuracy':[],'1':{'precision':[], 'recall':[], 'f1-score':[]}}\n",
    "\n",
    "    # creating dataframe \n",
    "    vectorizer = vectorizertype(min_df=2, max_df=0.5, ngram_range=(1,ngram))\n",
    "    X = vectorizer.fit_transform(X1)\n",
    "    DF = pd.DataFrame(X.todense(),columns=vectorizer.get_feature_names())\n",
    "    DF['Y'] = labels_y\n",
    "    X = DF.drop('Y', axis=1)\n",
    "    y = DF['Y']\n",
    "\n",
    "    # feature selection\n",
    "    def chi_square(X, y): \n",
    "        F, pval = chi2(X, y) \n",
    "        return F\n",
    "    def feature_ranking(F):\n",
    "        idx = np.argsort(F)\n",
    "        return idx[::-1]\n",
    "    df = pd.DataFrame(feature_ranking(chi_square(X,y)))\n",
    "    df.columns = ['column_numbers']\n",
    "    clms_toselect = df['column_numbers'].head(1000)\n",
    "    a = [DF.columns[i] for i in clms_toselect]\n",
    "    X1 = DF[a]\n",
    "    X1 = X1.to_numpy()\n",
    "    \n",
    "    # random splits X1a and X1b\n",
    "    np.random.shuffle(np.transpose(X1))\n",
    "    X1_split = np.hsplit(X1, 2)\n",
    "    X1a = X1_split[0]\n",
    "    X1b = X1_split[1]\n",
    "\n",
    "    # initialize base classifiers \n",
    "    SVM = SVC(probability=True, kernel='rbf', gamma='scale', C=9)\n",
    "    NB = MultinomialNB(alpha=0.7)\n",
    "    RF = RandomForestClassifier(n_estimators=150, max_depth=30, min_samples_split=20)\n",
    "    \n",
    "    # KFold crossvalidation\n",
    "    kf = KFold(n_splits=nsplits, shuffle=False)\n",
    "    for train_index, test_index, in kf.split(X1):\n",
    "        X1_train, X1_test = X1a[train_index], X1a[test_index]     # pick X1a or X1\n",
    "        X2_train, X2_test = X1b[train_index], X1b[test_index]     # pick X1b or X2\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # removing labels    \n",
    "        rng = np.random.RandomState(42) \n",
    "        random_unlabeled_points = rng.rand(len(y_train)) < ratio\n",
    "        labels = np.copy(y_train)\n",
    "        labels[random_unlabeled_points] = -1\n",
    "        y_train = labels\n",
    "\n",
    "        # train models\n",
    "        cotraining_model_SVM = CoTrainingClassifier(SVM)\n",
    "        cotraining_model_SVM.fit(X1_train, X2_train, y_train)\n",
    "\n",
    "        cotraining_model_NB = CoTrainingClassifier(NB)\n",
    "        cotraining_model_NB.fit(X1_train, X2_train, y_train)\n",
    "\n",
    "        cotraining_model_RF = CoTrainingClassifier(RF)\n",
    "        cotraining_model_RF.fit(X1_train, X2_train, y_train)\n",
    "\n",
    "        # evaluate models\n",
    "        y_pred_SVM = cotraining_model_SVM.predict(X1_test, X2_test)\n",
    "        y_pred_NB = cotraining_model_NB.predict(X1_test, X2_test)\n",
    "        y_pred_RF = cotraining_model_RF.predict(X1_test, X2_test)\n",
    "        \n",
    "        report = classification_report(y_test,y_pred_SVM, output_dict=True)\n",
    "        CT_SVM_results['accuracy'] += [report['accuracy']] \n",
    "        CT_SVM_results['1']['precision'] += [report['1']['precision']]\n",
    "        CT_SVM_results['1']['recall'] += [report['1']['recall']]\n",
    "        CT_SVM_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "   \n",
    "        report = classification_report(y_test,y_pred_NB, output_dict=True)      \n",
    "        CT_NB_results['accuracy'] += [report['accuracy']]\n",
    "        CT_NB_results['1']['precision'] += [report['1']['precision']]\n",
    "        CT_NB_results['1']['recall'] += [report['1']['recall']]\n",
    "        CT_NB_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "\n",
    "        report = classification_report(y_test,y_pred_RF, output_dict=True)   \n",
    "        CT_RF_results['accuracy'] += [report['accuracy']] \n",
    "        CT_RF_results['1']['precision'] += [report['1']['precision']]\n",
    "        CT_RF_results['1']['recall'] += [report['1']['recall']]\n",
    "        CT_RF_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "        \n",
    "    with open('Co_Training_results.txt', 'a') as file:\n",
    "        file.write('___ratio:' + str(ratio) + '|ngram:' + str(ngram) + '|vectorizer:'+ str(vectorizertype) + '___\\n')\n",
    "        file.write('SVM:\\n')\n",
    "        file.write('accuracy: ' + str(round(mean(CT_SVM_results['accuracy']),2)) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(CT_SVM_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(CT_SVM_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(CT_SVM_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(CT_SVM_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "\n",
    "        file.write('NB:\\n')        \n",
    "        file.write('accuracy: ' + str(round(mean(CT_NB_results['accuracy']),2)) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(CT_NB_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(CT_NB_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(CT_NB_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(CT_NB_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "        \n",
    "        file.write('RF:\\n')        \n",
    "        file.write('accuracy: ' + str(round(mean(CT_RF_results['accuracy']),2)) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(CT_RF_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(CT_RF_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(CT_RF_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(CT_RF_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "        obj = datetime.now()\n",
    "        file.write(str(obj) + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_cotraining_CV(ratio=0.80, vectorizertype=CountVectorizer, ngram=1, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
