{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTING FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads files and creates list\n",
    "def filereader(path):\n",
    "    list_of_comments = []\n",
    "    all_files = os.listdir(path)\n",
    "    for folder in all_files:\n",
    "        files = os.listdir(path+folder)\n",
    "        for file in files:\n",
    "            pathname = path+folder+'\\\\'+file\n",
    "            file = open(pathname, 'r')\n",
    "            list_of_comments.append(file.readline())\n",
    "    return list_of_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ott et. al (2011) - MTurk Hotel dataset\n",
    "neg_pol_truthful = filereader(\"C:\\\\Users\\\\alexa\\\\Desktop\\\\op_spam_dataset\\\\negative_polarity\\\\truthful_from_Web\\\\\")\n",
    "neg_pol_deceptive = filereader(\"C:\\\\Users\\\\alexa\\\\Desktop\\\\op_spam_dataset\\\\negative_polarity\\\\deceptive_from_MTurk\\\\\")\n",
    "pos_pol_truthful = filereader(\"C:\\\\Users\\\\alexa\\\\Desktop\\\\op_spam_dataset\\\\positive_polarity\\\\truthful_from_TripAdvisor\\\\\")\n",
    "pos_pol_deceptive = filereader(\"C:\\\\Users\\\\alexa\\\\Desktop\\\\op_spam_dataset\\\\positive_polarity\\\\deceptive_from_MTurk\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mukherjee et. al (2013) - Yelp Hotel dataset\n",
    "df = pd.read_csv(r'C:\\Users\\alexa\\Desktop\\YelpReviews.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle and reset index\n",
    "\n",
    "#creating deceptive lists\n",
    "deceptive = df[df['flagged']=='Y']\n",
    "pos_dec = deceptive[deceptive['rating'] > 2]\n",
    "neg_dec = deceptive[deceptive['rating'] < 3] \n",
    "pos_pol_deceptive = [str(review) for review in pos_dec['reviewContent']]\n",
    "pos_pol_deceptive = [review.replace(u'\\xa0', u' ') for review in pos_pol_deceptive]\n",
    "neg_pol_deceptive = [str(review) for review in neg_dec['reviewContent']]\n",
    "neg_pol_deceptive = [review.replace(u'\\xa0', u' ') for review in neg_pol_deceptive]\n",
    "\n",
    "# creating truthful lists\n",
    "truthful = df[df['flagged']=='N'][:780] #shuffled before, so random selection for balanced dataset\n",
    "pos_tru = truthful[truthful['rating'] > 2]\n",
    "neg_tru = truthful[truthful['rating'] < 3]\n",
    "pos_pol_truthful = [str(review) for review in pos_tru['reviewContent']]\n",
    "pos_pol_truthful = [review.replace(u'\\xa0', u' ') for review in pos_pol_truthful]\n",
    "neg_pol_truthful = [str(review) for review in neg_tru['reviewContent']]\n",
    "neg_pol_truthful = [review.replace(u'\\xa0', u' ') for review in neg_pol_truthful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rayana (2015) - Yelp Restaurant dataset\n",
    "\n",
    "#importing content\n",
    "file = open(\"reviewContent\", \"rb\")\n",
    "content = file.readlines()\n",
    "newlist = []\n",
    "for file in content[:5000]:\n",
    "    decoded = file.decode(\"utf-8\")\n",
    "    splitted = decoded.split('\\t')\n",
    "    splitted[3].replace(u'\\xa0', u' ')\n",
    "    newlist += [splitted]\n",
    "    \n",
    "#importing labels\n",
    "file2 = open(\"metadata\", \"rb\")\n",
    "content2 = file2.readlines()\n",
    "newlist2 = []\n",
    "for file in content2[:5000]:\n",
    "    decoded = file.decode(\"utf-8\")\n",
    "    splitted = decoded.split('\\t')\n",
    "    newlist2 += [splitted]\n",
    "\n",
    "#creating dataframe\n",
    "df = pd.DataFrame(newlist)\n",
    "df = df.drop([0,1,2], 1)\n",
    "df.columns = ['content']\n",
    "\n",
    "df2 = pd.DataFrame(newlist2)\n",
    "df2 = df2.drop([0,1,4], 1)\n",
    "df2.columns = ['rating', 'label']\n",
    "df = df.join(df2)\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle and reset index\n",
    "df = df.astype({\"rating\": float})\n",
    "\n",
    "#creating deceptive lists\n",
    "deceptive = df.loc[df['label'] == '-1'][:800] #shuffles before, so random\n",
    "pos_dec = deceptive[deceptive['rating'] > 2]\n",
    "neg_dec = deceptive[deceptive['rating'] < 3] \n",
    "pos_pol_deceptive = [str(review) for review in pos_dec['content']]\n",
    "neg_pol_deceptive = [str(review) for review in neg_dec['content']]\n",
    "\n",
    "# creating truthful lists\n",
    "truthful = df.loc[df['label'] == '1'][:800]\n",
    "pos_tru = truthful[truthful['rating'] > 2]\n",
    "neg_tru = truthful[truthful['rating'] < 3]\n",
    "pos_pol_truthful = [str(review) for review in pos_tru['content']]\n",
    "neg_pol_truthful = [str(review) for review in neg_tru['content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, pos_tags=False):\n",
    "    # tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]   \n",
    "\n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    # remove words less than three characters\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    # remove capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # lemmatizing\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # includes POS_tags\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "        tokens = ['_'.join(t) for t in tokens]\n",
    "    \n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text \n",
    "\n",
    "def preprocesser(list_of_texts): # turning lists into preprocessed lists\n",
    "    new = []\n",
    "    for item in list_of_texts:\n",
    "        new += [preprocessing(item)]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling data: 0 for truthful, 1 for deceptive\n",
    "corpus_neg_truthful = [(r,0) for r in preprocesser(neg_pol_truthful)]\n",
    "corpus_neg_deceptive = [(r,1) for r in preprocesser(neg_pol_deceptive)]\n",
    "corpus_pos_truthful = [(r,0) for r in preprocesser(pos_pol_truthful)]\n",
    "corpus_pos_deceptive = [(r,1) for r in preprocesser(pos_pol_deceptive)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data instances\n",
    "from random import shuffle\n",
    "corpus = corpus_neg_truthful+corpus_neg_deceptive+corpus_pos_truthful+corpus_pos_deceptive\n",
    "shuffle(corpus)\n",
    "reviews = [r for r,l in corpus]\n",
    "labels_y = [l for r,l in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELF TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from self_learning.frameworks.SelfLearning import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from statistics import mean, stdev\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_learning.methods.scikitTSVM import *\n",
    "from self_learning.methods.qns3vm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_TSVM_CV(ratio, vectorizertype, ngram, nsplits):\n",
    "   \n",
    "    \"\"\"\n",
    "    Main function with cross-validation included\n",
    "    Parameters:\n",
    "        ratio (number of labels to remove) = float (0.80 / 0.90 / 0.95)\n",
    "        vectorizertype (word counts or TF-IDF values) = CountVectorizer / TfidfVectorizer\n",
    "        ngram (word pairs) = int (1=unigram, 2=bigram)\n",
    "        nsplits (cross validation splits) = int (number of splits)\n",
    "    \n",
    "    \"\"\"\n",
    "    TSVM_results = {'accuracy':[], '1':{'precision':[], 'recall':[], 'f1-score':[]}}\n",
    "\n",
    "    \n",
    "    # creating dataframe \n",
    "    vectorizer = vectorizertype(min_df=2, max_df=0.5, ngram_range=(1,ngram))\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    DF = pd.DataFrame(X.todense(),columns=vectorizer.get_feature_names())\n",
    "    DF['Y'] = labels_y\n",
    "    X = DF.drop('Y', axis=1)\n",
    "    y = DF['Y']\n",
    "    \n",
    "    # feature selection\n",
    "    def chi_square(X, y): \n",
    "        F, pval = chi2(X, y) \n",
    "        return F\n",
    "    def feature_ranking(F):\n",
    "        idx = np.argsort(F)\n",
    "        return idx[::-1]\n",
    "    df = pd.DataFrame(feature_ranking(chi_square(X,y)))\n",
    "    df.columns = ['column_numbers']\n",
    "    clms_toselect = df['column_numbers'].head(1000)\n",
    "    a = [DF.columns[i] for i in clms_toselect]\n",
    "    X = DF[a]\n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    # KFold crossvalidation\n",
    "    kf = KFold(n_splits=nsplits, shuffle=False)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # removing labels    \n",
    "        rng = np.random.RandomState(42) \n",
    "        random_unlabeled_points = rng.rand(len(y_train)) < ratio\n",
    "        labels = np.copy(y_train)\n",
    "        labels[random_unlabeled_points] = -1\n",
    "        y_train = labels\n",
    "\n",
    "        # train model\n",
    "        tsvm_model = SKTSVM(kernel='rbf',C=0.0002, gamma=20) #WRAPS QN3SVM\n",
    "        tsvm_model.fit(X_train, y_train)\n",
    "        \n",
    "        # evaluate model\n",
    "        y_pred = tsvm_model.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test,y_pred, output_dict=True)\n",
    "        TSVM_results['accuracy'] += [report['accuracy']]\n",
    "        TSVM_results['1']['precision'] += [report['1']['precision']]\n",
    "        TSVM_results['1']['recall'] += [report['1']['recall']]\n",
    "        TSVM_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "        \n",
    "    with open('TSVM_results.txt', 'a') as file:\n",
    "        file.write('___ratio:' + str(ratio) + '|ngram:' + str(ngram) + '|vectorizer:'+ str(vectorizertype) + '___\\n')\n",
    "        file.write('accuracy: ' + str(round(mean(TSVM_results['accuracy']),2)) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(TSVM_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(TSVM_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(TSVM_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(TSVM_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "        obj = datetime.now()\n",
    "        file.write(str(obj) + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "PendingDeprecationWarning",
     "evalue": "the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPendingDeprecationWarning\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0d1a9b9f81fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain_TSVM_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizertype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsplits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-b13a07a7ba5f>\u001b[0m in \u001b[0;36mmain_TSVM_CV\u001b[1;34m(ratio, vectorizertype, ngram, nsplits)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mtsvm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSKTSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#WRAPS QN3SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mtsvm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\self_learning\\methods\\scikitTSVM.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQN_S3VM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeledX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeledy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munlabeledX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlamU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# probabilities by Platt scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\qns3vm.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mcomputed\u001b[0m \u001b[0mpartition\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munlabeled\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \"\"\"\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_valued\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\qns3vm.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mcomputed\u001b[0m \u001b[0mpartition\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munlabeled\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mindi_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__optimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__recomputeModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindi_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getTrainingPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\qns3vm.py\u001b[0m in \u001b[0;36m__optimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting optimization with BFGS ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__needed_function_calls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__initializeMatrices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;31m# starting point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mc_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\qns3vm.py\u001b[0m in \u001b[0;36m__initializeMatrices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__L_l\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__YL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__YL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__YL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Initialize kernel matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36masmatrix\u001b[1;34m(data, dtype)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(subtype, data, dtype, copy)\u001b[0m\n\u001b[0;32m    121\u001b[0m                       \u001b[1;34m'numpy-for-matlab-users.html). '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                       \u001b[1;34m'Please adjust your code to use regular ndarray.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                       PendingDeprecationWarning, stacklevel=2)\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mdtype2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPendingDeprecationWarning\u001b[0m: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray."
     ]
    }
   ],
   "source": [
    "main_TSVM_CV(ratio=0.80, vectorizertype=CountVectorizer, ngram=1, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_selftraining_CV(ratio, vectorizertype, ngram, nsplits):\n",
    "   \n",
    "    \"\"\"\n",
    "    Main function with cross-validation included\n",
    "    Parameters:\n",
    "        ratio (number of labels to remove) = float (0.80 / 0.90 / 0.95)\n",
    "        vectorizertype (word counts or TF-IDF values) = CountVectorizer / TfidfVectorizer\n",
    "        ngram (word pairs) = int (1=unigram, 2=bigram)\n",
    "        nsplits (cross validation splits) = int (number of splits)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize results dictionary \n",
    "    SL_SVM_results = {'accuracy':[], '1':{'precision':[], 'recall':[], 'f1-score':[]}}\n",
    "    SL_NB_results = {'accuracy':[], '1':{'precision':[], 'recall':[], 'f1-score':[]}}    \n",
    "    SL_RF_results = {'accuracy':[],'1':{'precision':[], 'recall':[], 'f1-score':[]}}\n",
    "    \n",
    "    # creating dataframe \n",
    "    vectorizer = vectorizertype(min_df=2, max_df=0.5, ngram_range=(1,ngram))\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    DF = pd.DataFrame(X.todense(),columns=vectorizer.get_feature_names())\n",
    "    DF['Y'] = labels_y\n",
    "    X = DF.drop('Y', axis=1)\n",
    "    y = DF['Y']\n",
    "    \n",
    "    # feature selection\n",
    "    def chi_square(X, y): \n",
    "        F, pval = chi2(X, y) \n",
    "        return F\n",
    "    def feature_ranking(F):\n",
    "        idx = np.argsort(F)\n",
    "        return idx[::-1]\n",
    "    df = pd.DataFrame(feature_ranking(chi_square(X,y)))\n",
    "    df.columns = ['column_numbers']\n",
    "    clms_toselect = df['column_numbers'].head(1000)\n",
    "    a = [DF.columns[i] for i in clms_toselect]\n",
    "    X = DF[a]\n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    # initialize base classifiers \n",
    "    if ratio == 0.95:\n",
    "        SVM = SVC(probability=True, kernel='rbf', gamma='scale', C=4)\n",
    "    elif ratio == 0.90:\n",
    "        SVM = SVC(probability=True, kernel='rbf', gamma='scale', C=9)\n",
    "    elif ratio == 0.80:\n",
    "        SVM = SVC(probability=True, kernel='rbf', gamma='scale', C=4)\n",
    "    else:\n",
    "        SVM = SVC(probability=True, kernel='rbf', gamma='scale', C=4)\n",
    "    \n",
    "    if ratio == 0.95:\n",
    "        NB = MultinomialNB(alpha=0.2)\n",
    "    elif ratio == 0.90:\n",
    "        NB = MultinomialNB(alpha=0.7)\n",
    "    elif ratio == 0.80:\n",
    "        NB = MultinomialNB(alpha=0.4)\n",
    "    else:\n",
    "        NB = MultinomialNB(alpha=0.7)\n",
    "#     NB = MultinomialNB(alpha=0.3)    \n",
    "        \n",
    "    if ratio == 0.95:\n",
    "        RF = RandomForestClassifier(n_estimators=150, max_depth=120, min_samples_split=30)\n",
    "    elif ratio == 0.90:\n",
    "        RF = RandomForestClassifier(n_estimators=150, max_depth=90, min_samples_split=40)\n",
    "    elif ratio == 0.80:\n",
    "        RF = RandomForestClassifier(n_estimators=150, max_depth=140, min_samples_split=30)\n",
    "    else:\n",
    "        RF = RandomForestClassifier(n_estimators=150, max_depth=90, min_samples_split=20)   \n",
    "    \n",
    "    \n",
    "    # KFold crossvalidation\n",
    "    kf = KFold(n_splits=nsplits, shuffle=False)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # removing labels    \n",
    "        rng = np.random.RandomState(42) \n",
    "        random_unlabeled_points = rng.rand(len(y_train)) < ratio\n",
    "        labels = np.copy(y_train)\n",
    "        labels[random_unlabeled_points] = -1\n",
    "        y_train = labels\n",
    "\n",
    "        # train models\n",
    "        self_learning_model_SVM = SelfLearningModel(SVM)\n",
    "        self_learning_model_SVM.fit(X_train, y_train)\n",
    "\n",
    "        self_learning_model_NB = SelfLearningModel(NB)\n",
    "        self_learning_model_NB.fit(X_train, y_train)\n",
    "\n",
    "        self_learning_model_RF = SelfLearningModel(RF)\n",
    "        self_learning_model_RF.fit(X_train, y_train)\n",
    "       \n",
    "        # evaluate models   \n",
    "        y_pred_SVM = self_learning_model_SVM.predict(X_test)\n",
    "        y_pred_NB = self_learning_model_NB.predict(X_test)\n",
    "        y_pred_RF = self_learning_model_RF.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test,y_pred_SVM, output_dict=True)\n",
    "        SL_SVM_results['accuracy'] += [report['accuracy']]\n",
    "        SL_SVM_results['1']['precision'] += [report['1']['precision']]\n",
    "        SL_SVM_results['1']['recall'] += [report['1']['recall']]\n",
    "        SL_SVM_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "        \n",
    "        report = classification_report(y_test,y_pred_NB, output_dict=True) \n",
    "        SL_NB_results['accuracy'] += [report['accuracy']] \n",
    "        SL_NB_results['1']['precision'] += [report['1']['precision']]\n",
    "        SL_NB_results['1']['recall'] += [report['1']['recall']]\n",
    "        SL_NB_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "        \n",
    "        report = classification_report(y_test,y_pred_RF, output_dict=True)        \n",
    "        SL_RF_results['accuracy'] += [report['accuracy']]\n",
    "        SL_RF_results['1']['precision'] += [report['1']['precision']]\n",
    "        SL_RF_results['1']['recall'] += [report['1']['recall']]\n",
    "        SL_RF_results['1']['f1-score'] += [report['1']['f1-score']]\n",
    "        \n",
    "        \n",
    "    # write results to text file\n",
    "    with open('Self_Training_results_dataset3.txt', 'a') as file:\n",
    "        file.write('___ratio:' + str(ratio) + '|ngram:' + str(ngram) + '|vectorizer:'+ str(vectorizertype) + '___\\n')\n",
    "        file.write('SVM:\\n')\n",
    "        file.write('accuracy: ' + str(round(mean(SL_SVM_results['accuracy']),2)) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(SL_SVM_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(SL_SVM_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(SL_SVM_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(SL_SVM_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "\n",
    "        file.write('NB:\\n')        \n",
    "        file.write('accuracy: ' + str(mean(SL_NB_results['accuracy'])) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(SL_NB_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(SL_NB_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(SL_NB_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(SL_NB_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "        \n",
    "        file.write('RF:\\n')        \n",
    "        file.write('accuracy: ' + str(round(mean(SL_RF_results['accuracy']),2)) + '\\n')\n",
    "        file.write('stddev: ' + str(stdev(SL_RF_results['accuracy'])) + '\\n')\n",
    "        file.write('precision: ' + str(round(mean(SL_RF_results['1']['precision']),2)) + '\\n')\n",
    "        file.write('recall: ' + str(round(mean(SL_RF_results['1']['recall']),2)) + '\\n')\n",
    "        file.write('F1 :' + str(round(mean(SL_RF_results['1']['f1-score']),2)) + '\\n\\n')\n",
    "        obj = datetime.now()\n",
    "        file.write(str(obj) + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main_selftraining_CV(ratio=0.80, vectorizertype=CountVectorizer, ngram=1, nsplits=5)\n",
    "print(1)\n",
    "main_selftraining_CV(ratio=0.90, vectorizertype=CountVectorizer, ngram=1, nsplits=5)\n",
    "print(2)\n",
    "main_selftraining_CV(ratio=0.95, vectorizertype=CountVectorizer, ngram=1, nsplits=5)\n",
    "print(3)\n",
    "main_selftraining_CV(ratio=0.80, vectorizertype=CountVectorizer, ngram=2, nsplits=5)\n",
    "print(4)\n",
    "main_selftraining_CV(ratio=0.90, vectorizertype=CountVectorizer, ngram=2, nsplits=5)\n",
    "print(5)\n",
    "main_selftraining_CV(ratio=0.95, vectorizertype=CountVectorizer, ngram=2, nsplits=5)\n",
    "print(6)\n",
    "main_selftraining_CV(ratio=0.80, vectorizertype=TfidfVectorizer, ngram=2, nsplits=5)\n",
    "print(7)\n",
    "main_selftraining_CV(ratio=0.90, vectorizertype=TfidfVectorizer, ngram=2, nsplits=5)\n",
    "print(8)\n",
    "main_selftraining_CV(ratio=0.95, vectorizertype=TfidfVectorizer, ngram=2, nsplits=5)\n",
    "print(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
